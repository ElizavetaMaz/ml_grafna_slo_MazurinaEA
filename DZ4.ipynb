{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-18T21:04:24.332672Z",
     "start_time": "2025-12-18T21:04:24.323216Z"
    }
   },
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T21:24:59.965134Z",
     "start_time": "2025-12-18T21:24:59.959957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# напишем requirements.txt\n",
    "requirements = \"\"\"\n",
    "scikit-learn==1.7.2\n",
    "joblib\n",
    "fastapi\n",
    "uvicorn\n",
    "numpy\n",
    "prometheus_client\n",
    "python-multipart\n",
    "\"\"\"\n",
    "\n",
    "with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"✅ requirements.txt создан:\")"
   ],
   "id": "4a3046b7ff632125",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ requirements.txt создан:\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T16:18:29.325442Z",
     "start_time": "2025-12-20T16:18:29.315442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# напишем наш мини API с эндпоинтами\n",
    "main = \"\"\"\n",
    "\n",
    "from fastapi import FastAPI, Response, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\n",
    "import time\n",
    "\n",
    "# Настройка логов\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Счётчик запросов\n",
    "requests_total = Counter(\"requests_total\", \"Total number of requests\", [\"endpoint\"])\n",
    "\n",
    "# Гистограмма для латентности (время выполнения запроса)\n",
    "request_latency = Histogram(\n",
    "    \"request_latency_seconds\",\n",
    "    \"Request latency in seconds\",\n",
    "    [\"endpoint\"]\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n",
    "# Загружаем модель (предварительно сохрани её через joblib.dump)\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models/model.pkl\")\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    start = time.time()\n",
    "    requests_total.labels(endpoint=\"/health\").inc()\n",
    "    logger.info(\"Зашли в Health\")\n",
    "    response = {\"status\": \"ok\", \"version\": VERSION}\n",
    "    request_latency.labels(endpoint=\"/health\").observe(time.time() - start)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/ui\", response_class=HTMLResponse)\n",
    "def ui_form():\n",
    "    start = time.time()\n",
    "    requests_total.labels(endpoint=\"/ui\").inc()\n",
    "    html_content = '''\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>ML Wine Prediction UI</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h2>Введите значения признаков для предсказания</h2>\n",
    "            <form action=\"/predict\" method=\"post\">\n",
    "                <label for=\"request\">Фичи (через запятую):</label><br>\n",
    "                <input type=\"text\" id=\"x\" name=\"request\" value=\"1.0,2.0,3.0\"><br><br>\n",
    "                <input type=\"submit\" value=\"Отправить\">\n",
    "            </form>\n",
    "        </body>\n",
    "    </html>\n",
    "    '''\n",
    "    request_latency.labels(endpoint=\"/ui\").observe(time.time() - start)\n",
    "    return HTMLResponse(content=html_content)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(request: str = Form(...)):\n",
    "    start = time.time()\n",
    "    requests_total.labels(endpoint=\"/predict\").inc()\n",
    "    features = [float(val.strip()) for val in request.split(',')]\n",
    "    logger.info(f\"Начинаю предикт для интпута: {features}\")\n",
    "    X = np.array(features).reshape(1, -1)\n",
    "    y_pred = model.predict(X)[0]\n",
    "    logger.info(f\"Получен предикт: {y_pred}\")\n",
    "    response = {\n",
    "        \"status\": \"ok\",\n",
    "        \"prediction\": int(y_pred),\n",
    "        \"version\": VERSION\n",
    "    }\n",
    "    request_latency.labels(endpoint=\"/predict\").observe(time.time() - start)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "\n",
    "@app.get(\"/sleep\")\n",
    "def health():\n",
    "    start = time.time()\n",
    "    requests_total.labels(endpoint=\"/sleep\").inc()\n",
    "    logger.info(\"Зашли в Sleep\")\n",
    "    time.sleep(2)\n",
    "    response = {\"status\": \"ok\", \"sleep\": \"ok\"}\n",
    "    request_latency.labels(endpoint=\"/sleep\").observe(time.time() - start)\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"app/main.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(main)\n",
    "\n",
    "print(\"✅ main.py создан:\")"
   ],
   "id": "6a7895628e2c2fb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ main.py создан:\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T21:59:41.000748Z",
     "start_time": "2025-12-18T21:59:40.990829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# создадим докер для запуска API с моделью\n",
    "dockerfile_content = \"\"\"\n",
    "# Базовый образ\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Переменные окружения\n",
    "ENV MODEL_PATH=/app/models/model.pkl \\\\\n",
    "    MODEL_VERSION=v1.0.0\n",
    "\n",
    "# Рабочая директория\n",
    "WORKDIR /app\n",
    "\n",
    "# Копирование зависимостей и установка\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Копируем весь проект\n",
    "COPY . .\n",
    "\n",
    "# Точка входа\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "\"\"\"\n",
    "\n",
    "dockerignore = \"\"\"\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".git\n",
    ".venv\n",
    "tests/\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content.strip())\n",
    "\n",
    "print(\"✅ Dockerfile создан:\")\n",
    "\n",
    "with open(\".dockerignore\", \"w\") as f:\n",
    "    f.write(dockerignore.strip())\n",
    "\n",
    "print(\"✅ .dockerignore создан:\")"
   ],
   "id": "295d3e7ba35249ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dockerfile создан:\n",
      "✅ .dockerignore создан:\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T14:55:44.829758Z",
     "start_time": "2025-12-20T14:55:44.819761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# создадим конфиг прометеуса, чтобы он мониторил метрики с api (в таргете ставим имя работы и порт так как мл сервис будет в докер композе)\n",
    "prometheus_content = \"\"\"\n",
    "scrape_configs:\n",
    "  - job_name: \"ml_service\"\n",
    "    static_configs:\n",
    "      - targets: [\"ml_service:8080\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"prometheus.yml\", \"w\") as f:\n",
    "    f.write(prometheus_content.strip())\n",
    "\n",
    "print(\"✅ prometheus.yml создан:\")\n"
   ],
   "id": "e95280a005b40e0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ prometheus.yml создан:\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T22:01:18.914811Z",
     "start_time": "2025-12-18T22:01:18.902676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Напишем докер композе для запуска 3 сервисов одновременно\n",
    "docker_compose_content = \"\"\"\n",
    "version: \"3.9\"\n",
    "services:\n",
    "    prometheus:\n",
    "      image: prom/prometheus:latest\n",
    "      volumes:\n",
    "        - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      ports:\n",
    "        - \"9090:9090\"\n",
    "\n",
    "    grafana:\n",
    "      image: grafana/grafana:latest\n",
    "      ports:\n",
    "        - \"3000:3000\"\n",
    "      depends_on:\n",
    "        - prometheus\n",
    "\n",
    "    ml_service:\n",
    "      image: ml-service-dz4-ml-in-prodaction:v1\n",
    "      ports:\n",
    "        - \"8000:8080\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"docker-compose.yml\", \"w\") as f:\n",
    "    f.write(docker_compose_content.strip())\n",
    "\n",
    "print(\"✅ docker-compose.yml создан:\")"
   ],
   "id": "39e2511c476f9b06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ docker-compose.yml создан:\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d6c47880aa7a76e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
